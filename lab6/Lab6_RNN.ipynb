{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dqb6_SeUfkG7"
   },
   "source": [
    "# Lab 6: RNNs\n",
    "## Goal:\n",
    "- Understand the mechanics of RNNs in Pytorch\n",
    "- Train RNN based neural networks on text data\n",
    "- Basics of word embedding and how to use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOjobxhbfkHA",
    "outputId": "8c13d622-0fc8-44d9-89fd-620c6480d7c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n",
      "Device being used: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuxuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device being used: %s\" %device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97W7LJNtfkHE",
    "toc-hr-collapsed": false
   },
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "id": "KqNi_76ofkHG",
    "outputId": "0a295c5e-37b8-4239-afba-d75790bd79df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>candidate</th>\n",
       "      <th>candidate_confidence</th>\n",
       "      <th>relevant_yn</th>\n",
       "      <th>relevant_yn_confidence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_confidence</th>\n",
       "      <th>subject_matter</th>\n",
       "      <th>subject_matter_confidence</th>\n",
       "      <th>candidate_gold</th>\n",
       "      <th>...</th>\n",
       "      <th>relevant_yn_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sentiment_gold</th>\n",
       "      <th>subject_matter_gold</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6578</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @NancyLeeGrahn: How did everyone feel about...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697200650592256</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Scott Walker</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.6333</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ScottWalker: Didn't catch the full #GOPdeb...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199560069120</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.6629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @TJMShow: No mention of Tamir Rice and the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:46 -0700</td>\n",
       "      <td>629697199312482304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>No candidate mentioned</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>0.7039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @RobGeorge: That Carly Fiorina is trending ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697197118861312</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.7045</td>\n",
       "      <td>None of the above</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @DanScavino: #GOPDebate w/ @realDonaldTrump...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-08-07 09:54:45 -0700</td>\n",
       "      <td>629697196967903232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arizona</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               candidate  candidate_confidence relevant_yn  \\\n",
       "0   1  No candidate mentioned                   1.0         yes   \n",
       "1   2            Scott Walker                   1.0         yes   \n",
       "2   3  No candidate mentioned                   1.0         yes   \n",
       "3   4  No candidate mentioned                   1.0         yes   \n",
       "4   5            Donald Trump                   1.0         yes   \n",
       "\n",
       "   relevant_yn_confidence sentiment  sentiment_confidence     subject_matter  \\\n",
       "0                     1.0   Neutral                0.6578  None of the above   \n",
       "1                     1.0  Positive                0.6333  None of the above   \n",
       "2                     1.0   Neutral                0.6629  None of the above   \n",
       "3                     1.0  Positive                1.0000  None of the above   \n",
       "4                     1.0  Positive                0.7045  None of the above   \n",
       "\n",
       "   subject_matter_confidence candidate_gold  ... relevant_yn_gold  \\\n",
       "0                     1.0000            NaN  ...              NaN   \n",
       "1                     1.0000            NaN  ...              NaN   \n",
       "2                     0.6629            NaN  ...              NaN   \n",
       "3                     0.7039            NaN  ...              NaN   \n",
       "4                     1.0000            NaN  ...              NaN   \n",
       "\n",
       "  retweet_count  sentiment_gold subject_matter_gold  \\\n",
       "0             5             NaN                 NaN   \n",
       "1            26             NaN                 NaN   \n",
       "2            27             NaN                 NaN   \n",
       "3           138             NaN                 NaN   \n",
       "4           156             NaN                 NaN   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  RT @NancyLeeGrahn: How did everyone feel about...         NaN   \n",
       "1  RT @ScottWalker: Didn't catch the full #GOPdeb...         NaN   \n",
       "2  RT @TJMShow: No mention of Tamir Rice and the ...         NaN   \n",
       "3  RT @RobGeorge: That Carly Fiorina is trending ...         NaN   \n",
       "4  RT @DanScavino: #GOPDebate w/ @realDonaldTrump...         NaN   \n",
       "\n",
       "               tweet_created            tweet_id  tweet_location  \\\n",
       "0  2015-08-07 09:54:46 -0700  629697200650592256             NaN   \n",
       "1  2015-08-07 09:54:46 -0700  629697199560069120             NaN   \n",
       "2  2015-08-07 09:54:46 -0700  629697199312482304             NaN   \n",
       "3  2015-08-07 09:54:45 -0700  629697197118861312           Texas   \n",
       "4  2015-08-07 09:54:45 -0700  629697196967903232             NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0                       Quito  \n",
       "1                         NaN  \n",
       "2                         NaN  \n",
       "3  Central Time (US & Canada)  \n",
       "4                     Arizona  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1111)\n",
    "\n",
    "df = pd.read_csv('./Sentiment.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JD_GW-x3fkHK"
   },
   "source": [
    "Let's first look at some basic intuition and stats of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-saQCovdfkHL",
    "outputId": "2875ca8e-4efe-447e-a88e-c1e21dcf7d88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @NancyLeeGrahn: How did everyone feel about the Climate Change question last night? Exactly. #GOPDebate'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data is a string of words\n",
    "df.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "GhFdZwoSfkHO",
    "outputId": "99e1bb8f-5bee-4f74-cd59-8a800a95f7ab"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Negative</th>\n",
       "      <td>8493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neutral</th>\n",
       "      <td>3142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive</th>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "Negative   8493\n",
       "Neutral    3142\n",
       "Positive   2236"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(df.groupby('sentiment').count()['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL4LqQE1fkHR"
   },
   "source": [
    "For simplicity, \n",
    "- we only use ```X = 'text'``` and ```y = 'Sentiment'``` from the original dataframe. \n",
    "- We only look at positive (1) and negative (0) tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "tiJMS1ZafkHT",
    "outputId": "648ad1e8-591b-4da4-893e-fa6b40ceacdc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text\n",
       "sentiment      \n",
       "0          8493\n",
       "1          2236"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['sentiment', 'text']]\n",
    "df = df[df['sentiment'] != 'Neutral']\n",
    "df['sentiment'] = [1 if s == \"Positive\" else 0 for s in df['sentiment']]\n",
    "df.groupby('sentiment').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "dF6uRJORfkHX",
    "outputId": "36907017-f6ed-492b-b9dc-a814110aa675"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>79.152858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.847142</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text\n",
       "sentiment           \n",
       "0          79.152858\n",
       "1          20.847142"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(df, test_size=0.10, random_state=42)\n",
    "train_data.index = np.arange(len(train_data))\n",
    "test_data.index = np.arange(len(test_data))\n",
    "train_data.groupby('sentiment').count().apply(lambda x: 100 * x / float(x.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZXy_udKfkHa"
   },
   "source": [
    "### Input representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxCfQbRKfkHb"
   },
   "source": [
    "#### Build vocabulary\n",
    "We need to build a vocabulary using words in our training data. Any words in the test set that are not in our vocabulary will be replaced with an ```<UNK>``` token. We will also add a ```<PAD>``` token as padding.\n",
    "\n",
    "For computational purposes, we'll only take words that appeared more than 3 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qaZ_8lblfkHc",
    "outputId": "ab540f2f-3800-4022-cc44-e702bcf6fdc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3069\n"
     ]
    }
   ],
   "source": [
    "UNK = \"<UNK>\"\n",
    "PAD = \"<PAD>\"\n",
    "\n",
    "def build_vocab(sentences, min_count=3, max_vocab=None):\n",
    "    \"\"\"\n",
    "    Build vocabulary from sentences (list of strings)\n",
    "    \"\"\"\n",
    "    # keep track of the number of appearance of each word\n",
    "    word_count = Counter()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Regular expression operations: [] (indicate a set of characters), \n",
    "        sentence = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', sentence)\n",
    "        word_count.update(word_tokenize(sentence.lower()))\n",
    "    \n",
    "    vocabulary = list([w for w in word_count if word_count[w] > min_count]) + [UNK, PAD]\n",
    "    indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "    return vocabulary, indices\n",
    "\n",
    "vocabulary, vocab_indices = build_vocab(train_data['text'])\n",
    "\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'NYU']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I love NYU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0,\n",
       " 'is': 1,\n",
       " 'great': 2,\n",
       " '-': 3,\n",
       " 'let': 4,\n",
       " 's': 5,\n",
       " 'have': 6,\n",
       " 'a': 7,\n",
       " 'bunch': 8,\n",
       " 'of': 9,\n",
       " 'rich': 10,\n",
       " 'men': 11,\n",
       " 'make': 12,\n",
       " 'decisions': 13,\n",
       " 'about': 14,\n",
       " 'plannedparenthood': 15,\n",
       " 'gopdebates': 16,\n",
       " '@': 17,\n",
       " 'megynkelly': 18,\n",
       " 'had': 19,\n",
       " 'jebbush': 20,\n",
       " 'trump': 21,\n",
       " 'was': 22,\n",
       " 'foxnews': 23,\n",
       " 'has': 24,\n",
       " 'changed': 25,\n",
       " 'for': 26,\n",
       " 'the': 27,\n",
       " 'worse': 28,\n",
       " 'tcot': 29,\n",
       " 'rt': 30,\n",
       " ':': 31,\n",
       " 'gopdebate': 32,\n",
       " 'show': 33,\n",
       " 'hands': 34,\n",
       " 'like': 35,\n",
       " 'st': 36,\n",
       " 'day': 37,\n",
       " 'school': 38,\n",
       " 'we': 39,\n",
       " 're': 40,\n",
       " 'very': 41,\n",
       " 'little': 42,\n",
       " 'too': 43,\n",
       " 'many': 44,\n",
       " '&': 45,\n",
       " 'amp': 46,\n",
       " ';': 47,\n",
       " 'i': 48,\n",
       " 'want': 49,\n",
       " 'driving': 50,\n",
       " 'his': 51,\n",
       " 'fellow': 52,\n",
       " 'patients': 53,\n",
       " 'around': 54,\n",
       " 'in': 55,\n",
       " '``': 56,\n",
       " 'one': 57,\n",
       " 'over': 58,\n",
       " 'http': 59,\n",
       " 't': 60,\n",
       " 'co': 61,\n",
       " 'monaeltahawy': 62,\n",
       " 'any': 63,\n",
       " 'candidate': 64,\n",
       " 'received': 65,\n",
       " 'word': 66,\n",
       " 'from': 67,\n",
       " 'god': 68,\n",
       " 'presidential': 69,\n",
       " 'hopefuls': 70,\n",
       " 'america': 71,\n",
       " 'christian': 72,\n",
       " 'brotherhood': 73,\n",
       " '…': 74,\n",
       " 'did': 75,\n",
       " 'not': 76,\n",
       " 'touch': 77,\n",
       " 'several': 78,\n",
       " 'important': 79,\n",
       " 'issues': 80,\n",
       " 'gets': 81,\n",
       " 'most': 82,\n",
       " 'time': 83,\n",
       " 'beginning': 84,\n",
       " 'end': 85,\n",
       " 'randpaul': 86,\n",
       " 'that': 87,\n",
       " 'he': 88,\n",
       " 'going': 89,\n",
       " 'to': 90,\n",
       " 'begin': 91,\n",
       " 'with': 92,\n",
       " 'on': 93,\n",
       " 'been': 94,\n",
       " 'so': 95,\n",
       " 'far': 96,\n",
       " 'race': 97,\n",
       " 'and': 98,\n",
       " '’': 99,\n",
       " 'change': 100,\n",
       " \"''\": 101,\n",
       " 'kaivanshroff': 102,\n",
       " 'biggest': 103,\n",
       " 'winner': 104,\n",
       " 'tonight': 105,\n",
       " 'away': 106,\n",
       " 'hillary': 107,\n",
       " 'clinton': 108,\n",
       " 'jessicavalenti': 109,\n",
       " 'they': 110,\n",
       " 'all': 111,\n",
       " 'need': 112,\n",
       " 'reform': 113,\n",
       " 'prolife': 114,\n",
       " 'gun': 115,\n",
       " 'who': 116,\n",
       " 'believe': 117,\n",
       " 'death': 118,\n",
       " 'war': 119,\n",
       " 'jeb': 120,\n",
       " 'er': 121,\n",
       " 'bush': 122,\n",
       " 'really': 123,\n",
       " 'perfect': 124,\n",
       " 'gop': 125,\n",
       " 'boring': 126,\n",
       " 'big': 127,\n",
       " 'gov': 128,\n",
       " 'junior': 129,\n",
       " 'executive': 130,\n",
       " 'special': 131,\n",
       " 'marymauldin': 132,\n",
       " 'hey': 133,\n",
       " 'how': 134,\n",
       " 'absolutely': 135,\n",
       " 'fearful': 136,\n",
       " 'are': 137,\n",
       " 'you': 138,\n",
       " 'sentedcruz': 139,\n",
       " 'm': 140,\n",
       " 'lol': 141,\n",
       " 'at': 142,\n",
       " 'refuse': 143,\n",
       " 'ask': 144,\n",
       " 'him': 145,\n",
       " 'questions': 146,\n",
       " 'gopdebat…': 147,\n",
       " 'donald': 148,\n",
       " 'got': 149,\n",
       " 'wall': 150,\n",
       " 'idea': 151,\n",
       " 'game': 152,\n",
       " 'golden': 153,\n",
       " 'girls': 154,\n",
       " 'sucks': 155,\n",
       " 'megyn': 156,\n",
       " 'kelly': 157,\n",
       " 'played': 158,\n",
       " 'female': 159,\n",
       " 'card': 160,\n",
       " 'it': 161,\n",
       " 'nothing': 162,\n",
       " 'debate': 163,\n",
       " 'https': 164,\n",
       " '_hankrearden': 165,\n",
       " 'well': 166,\n",
       " 'much': 167,\n",
       " 'krauthammer': 168,\n",
       " 'rwsurfergirl': 169,\n",
       " 'posed': 170,\n",
       " 'adult': 171,\n",
       " 'pictures': 172,\n",
       " 'should': 173,\n",
       " 'bring': 174,\n",
       " 'up': 175,\n",
       " 'chucknellis': 176,\n",
       " 'when': 177,\n",
       " 'tedcruz': 178,\n",
       " 'talks': 179,\n",
       " 'such': 180,\n",
       " 'similar': 181,\n",
       " 'election': 182,\n",
       " 'real': 183,\n",
       " 'conservative': 184,\n",
       " 'answer': 185,\n",
       " 'go…': 186,\n",
       " 'disappointed': 187,\n",
       " 'my': 188,\n",
       " 'media': 189,\n",
       " 'after': 190,\n",
       " 'think': 191,\n",
       " 'realdonaldtrump': 192,\n",
       " 'htt…': 193,\n",
       " 'am': 194,\n",
       " 'wondering': 195,\n",
       " 'what': 196,\n",
       " 'fox': 197,\n",
       " '--': 198,\n",
       " 'get': 199,\n",
       " 'rid': 200,\n",
       " 'paul': 201,\n",
       " 'cruz': 202,\n",
       " 'carson': 203,\n",
       " '🇺🇸': 204,\n",
       " 'supermanhotmale': 205,\n",
       " 'dear': 206,\n",
       " 'walker': 207,\n",
       " 'brag': 208,\n",
       " 'fucked': 209,\n",
       " 'your': 210,\n",
       " 'state': 211,\n",
       " 'wisconsin': 212,\n",
       " 'suck': 213,\n",
       " 'koch': 214,\n",
       " 'bitchasspoli…': 215,\n",
       " 'out': 216,\n",
       " 'goldietaylor': 217,\n",
       " 'huckabee': 218,\n",
       " 'zy': 219,\n",
       " 'bdukx': 220,\n",
       " 'purpose': 221,\n",
       " 'military': 222,\n",
       " 'kill': 223,\n",
       " 'people': 224,\n",
       " 'break': 225,\n",
       " 'things': 226,\n",
       " 'thought': 227,\n",
       " 'national': 228,\n",
       " 'security': 229,\n",
       " 'matthews': 230,\n",
       " 'ass': 231,\n",
       " 'handed': 232,\n",
       " 'by': 233,\n",
       " 'carly': 234,\n",
       " 'fiorina': 235,\n",
       " 'h': 236,\n",
       " 'p': 237,\n",
       " 'thanks': 238,\n",
       " 'news': 239,\n",
       " 'raising': 240,\n",
       " 'ratings': 241,\n",
       " 'shows': 242,\n",
       " 'wrong': 243,\n",
       " 'fix': 244,\n",
       " 'bretbaier': 245,\n",
       " 'shannonrwatts': 246,\n",
       " 'states': 247,\n",
       " 'background': 248,\n",
       " 'check': 249,\n",
       " 'police': 250,\n",
       " 'cut': 251,\n",
       " 'almost': 252,\n",
       " 'half': 253,\n",
       " 'gop…': 254,\n",
       " 'watching': 255,\n",
       " 'abortion': 256,\n",
       " 'gender': 257,\n",
       " 'johnkasich': 258,\n",
       " 'didn': 259,\n",
       " 'back': 260,\n",
       " 'ohio': 261,\n",
       " 'medicaid': 262,\n",
       " 'expansion': 263,\n",
       " 'during': 264,\n",
       " 'last': 265,\n",
       " 'night': 266,\n",
       " 'don': 267,\n",
       " 'oppose': 268,\n",
       " 'based': 269,\n",
       " 'official': 270,\n",
       " 'photo': 271,\n",
       " 'obviously': 272,\n",
       " 'trying': 273,\n",
       " 'influence': 274,\n",
       " 'makeup': 275,\n",
       " 'republican': 276,\n",
       " 'field': 277,\n",
       " 'thoughts': 278,\n",
       " 'perry': 279,\n",
       " 'must': 280,\n",
       " 'replace': 281,\n",
       " 'kasich': 282,\n",
       " 'top': 283,\n",
       " 'ten': 284,\n",
       " 'jindal': 285,\n",
       " 'needs': 286,\n",
       " 'step': 287,\n",
       " 'christie': 288,\n",
       " 'there': 289,\n",
       " 'difference': 290,\n",
       " 'between': 291,\n",
       " 'being': 292,\n",
       " 'politically': 293,\n",
       " 'correct': 294,\n",
       " 'massive': 295,\n",
       " 'jerk': 296,\n",
       " 'mr': 297,\n",
       " 'women': 298,\n",
       " 'start': 299,\n",
       " 'protesting': 300,\n",
       " 'misogyny': 301,\n",
       " 'stay': 302,\n",
       " 'vagina': 303,\n",
       " 'unless': 304,\n",
       " 'placards': 305,\n",
       " 'va': 306,\n",
       " 'our': 307,\n",
       " 'veterans': 308,\n",
       " 'smethanie': 309,\n",
       " 'sure': 310,\n",
       " 'told': 311,\n",
       " 'me': 312,\n",
       " 'run': 313,\n",
       " 'agreed': 314,\n",
       " 'voted': 315,\n",
       " 'yes': 316,\n",
       " 'here': 317,\n",
       " 'could': 318,\n",
       " 've': 319,\n",
       " 'asked': 320,\n",
       " 'entire': 321,\n",
       " 'minutes': 322,\n",
       " 'instead': 323,\n",
       " 'an': 324,\n",
       " 'ted': 325,\n",
       " 'though': 326,\n",
       " 'auhlnza': 327,\n",
       " 'ww': 328,\n",
       " 'wage': 329,\n",
       " 'poor': 330,\n",
       " 'die': 331,\n",
       " 'now': 332,\n",
       " 'foreign': 333,\n",
       " 'policy': 334,\n",
       " 'huge': 335,\n",
       " 'f': 336,\n",
       " 'king': 337,\n",
       " 'donaldtrump': 338,\n",
       " 'faith': 339,\n",
       " 'scott': 340,\n",
       " 'pro-life': 341,\n",
       " 'if': 342,\n",
       " 'would': 343,\n",
       " 'woman': 344,\n",
       " 'rather': 345,\n",
       " 'than': 346,\n",
       " 'receive': 347,\n",
       " 'beg': 348,\n",
       " 'supporters': 349,\n",
       " 'easily': 350,\n",
       " 'tlot': 351,\n",
       " 'wakeupamerica': 352,\n",
       " 'd': 353,\n",
       " 'see': 354,\n",
       " 'long': 355,\n",
       " 'each': 356,\n",
       " 'given': 357,\n",
       " 'speak': 358,\n",
       " 'seem': 359,\n",
       " 'fair': 360,\n",
       " 'balanced': 361,\n",
       " 'were': 362,\n",
       " 'mess': 363,\n",
       " 'why': 364,\n",
       " 'dc': 365,\n",
       " 'band': 366,\n",
       " 'together': 367,\n",
       " 'expose': 368,\n",
       " 'set': 369,\n",
       " 'job': 370,\n",
       " 'rubio': 371,\n",
       " 'g…': 372,\n",
       " 'won': 373,\n",
       " 'admit': 374,\n",
       " 'leader': 375,\n",
       " 'right': 376,\n",
       " 'mean': 377,\n",
       " 'only': 378,\n",
       " 'double-digit': 379,\n",
       " 'lead': 380,\n",
       " 'gopd…': 381,\n",
       " 'jamiaw': 382,\n",
       " '-mike': 383,\n",
       " 'kkkorgop': 384,\n",
       " 'order': 385,\n",
       " 'another': 386,\n",
       " 'green': 387,\n",
       " 'apple': 388,\n",
       " 'martini': 389,\n",
       " 'process': 390,\n",
       " 'hell': 391,\n",
       " 'just': 392,\n",
       " 'happened': 393,\n",
       " 'talkpoverty': 394,\n",
       " 'oh': 395,\n",
       " 'e…': 396,\n",
       " 'pimps': 397,\n",
       " 'drug': 398,\n",
       " 'dealers': 399,\n",
       " 'once': 400,\n",
       " 'voxdotcom': 401,\n",
       " 'spot': 402,\n",
       " 'foxnewsdebate': 403,\n",
       " 'ig': 404,\n",
       " 'r': 405,\n",
       " 't…': 406,\n",
       " 'never': 407,\n",
       " 'know': 408,\n",
       " 'frontrunner': 409,\n",
       " 'straight': 410,\n",
       " 'c': 411,\n",
       " 'c…': 412,\n",
       " 'jr': 413,\n",
       " 'new': 414,\n",
       " 'graham': 415,\n",
       " 'slogan': 416,\n",
       " 'can': 417,\n",
       " 'wait': 418,\n",
       " 'clear': 419,\n",
       " 'federal': 420,\n",
       " 'government': 421,\n",
       " 'shouldn': 422,\n",
       " 'be': 423,\n",
       " 'involved': 424,\n",
       " 'education': 425,\n",
       " 'do': 426,\n",
       " 'gopde…': 427,\n",
       " 'writeintrump': 428,\n",
       " 'megan': 429,\n",
       " 'attacking': 430,\n",
       " 'week': 431,\n",
       " 'worked': 432,\n",
       " 'her': 433,\n",
       " 'running': 434,\n",
       " 'other': 435,\n",
       " 'candidates': 436,\n",
       " 'says': 437,\n",
       " 'no': 438,\n",
       " 'abt': 439,\n",
       " 'immigration': 440,\n",
       " 'until': 441,\n",
       " 'came': 442,\n",
       " 'along': 443,\n",
       " 'true': 444,\n",
       " 'figure': 445,\n",
       " 'times': 446,\n",
       " 'them': 447,\n",
       " 'foxdebate': 448,\n",
       " 'ridiculous': 449,\n",
       " 'bashing': 450,\n",
       " 'ignoring': 451,\n",
       " 'she': 452,\n",
       " 'doesn': 453,\n",
       " 'belong': 454,\n",
       " 'gay': 455,\n",
       " 'marriage': 456,\n",
       " 'reasonable': 457,\n",
       " 'someone': 458,\n",
       " 'person': 459,\n",
       " 'win': 460,\n",
       " 'close': 461,\n",
       " 'eyes': 462,\n",
       " 'try': 463,\n",
       " 'find': 464,\n",
       " 'saying': 465,\n",
       " 'marco': 466,\n",
       " 'waiting': 467,\n",
       " 'loser': 468,\n",
       " 'commercial': 469,\n",
       " 'thing': 470,\n",
       " 'record': 471,\n",
       " 'jobs': 472,\n",
       " 'w': 473,\n",
       " '$': 474,\n",
       " 'welfare': 475,\n",
       " 'immediately': 476,\n",
       " 'circle': 477,\n",
       " 'will': 478,\n",
       " 'iran': 479,\n",
       " 'taking': 480,\n",
       " 'health': 481,\n",
       " 'care': 482,\n",
       " 'm…': 483,\n",
       " 'ain': 484,\n",
       " 'bout': 485,\n",
       " 'dem': 486,\n",
       " 'ol': 487,\n",
       " 'abortions': 488,\n",
       " 'question': 489,\n",
       " 'vote': 490,\n",
       " 'guy': 491,\n",
       " 'gon': 492,\n",
       " 'na': 493,\n",
       " 'chill': 494,\n",
       " 'showed': 495,\n",
       " 'color': 496,\n",
       " 'basically': 497,\n",
       " 'anyone': 498,\n",
       " 'answering': 499,\n",
       " 'first': 500,\n",
       " 'straighten': 501,\n",
       " 'tries': 502,\n",
       " 'invade': 503,\n",
       " 'church': 504,\n",
       " 'funny': 505,\n",
       " 'o': 506,\n",
       " 'trumpeffect': 507,\n",
       " 'aware': 508,\n",
       " 'making': 509,\n",
       " 'sense': 510,\n",
       " 'attack': 511,\n",
       " 'doing': 512,\n",
       " 'stoping': 513,\n",
       " 'speaking': 514,\n",
       " 'gopdebate…': 515,\n",
       " 'everyone': 516,\n",
       " 'still': 517,\n",
       " 'talking': 518,\n",
       " 'realbencarson': 519,\n",
       " 'articulate': 520,\n",
       " 'good': 521,\n",
       " 'points': 522,\n",
       " 'made': 523,\n",
       " 'll': 524,\n",
       " 'pattonoswalt': 525,\n",
       " 'dr': 526,\n",
       " 'ben': 527,\n",
       " 'vision': 528,\n",
       " 'future': 529,\n",
       " 'tithing': 530,\n",
       " 'nielsen': 531,\n",
       " 'release': 532,\n",
       " 'afternoon': 533,\n",
       " 'viewers': 534,\n",
       " 'share': 535,\n",
       " 'reminds': 536,\n",
       " 'elevator': 537,\n",
       " 'music': 538,\n",
       " 'hear': 539,\n",
       " 'but': 540,\n",
       " 'listen': 541,\n",
       " 'alexandraheuser': 542,\n",
       " 'prime': 543,\n",
       " 'drama': 544,\n",
       " 'nd': 545,\n",
       " 'winners': 546,\n",
       " 'losers': 547,\n",
       " 'nights': 548,\n",
       " 'marthamaccallum': 549,\n",
       " 'wild': 550,\n",
       " 'makeamericagreatagain': 551,\n",
       " 'waynedupreeshow': 552,\n",
       " 'woke': 553,\n",
       " 'tweet': 554,\n",
       " 'best': 555,\n",
       " 'line': 556,\n",
       " 'via': 557,\n",
       " 'govmikehuckabee': 558,\n",
       " 'illegals': 559,\n",
       " 'country': 560,\n",
       " 'chris': 561,\n",
       " 'wallace': 562,\n",
       " 'asks': 563,\n",
       " 'proof': 564,\n",
       " 'shame': 565,\n",
       " 'obama': 566,\n",
       " 'does': 567,\n",
       " 'loving': 568,\n",
       " 'msgoddessrises': 569,\n",
       " 'ftw': 570,\n",
       " 'ron': 571,\n",
       " 'nailed': 572,\n",
       " 'telling': 573,\n",
       " 'shut': 574,\n",
       " 'u': 575,\n",
       " 'vmqlud': 576,\n",
       " 'thebaxterbean': 577,\n",
       " 'canadian': 578,\n",
       " 'citizenship': 579,\n",
       " 'might': 580,\n",
       " 'pick': 581,\n",
       " 'president': 582,\n",
       " 'gave': 583,\n",
       " 'some': 584,\n",
       " 'answers': 585,\n",
       " 'stop': 586,\n",
       " 'illegal': 587,\n",
       " 'closing': 588,\n",
       " 'man': 589,\n",
       " 'impressive': 590,\n",
       " 'lindasuhler': 591,\n",
       " 'stands': 592,\n",
       " 'act': 593,\n",
       " 'love': 594,\n",
       " 'comments': 595,\n",
       " 'earned': 596,\n",
       " 'legal': 597,\n",
       " 'status': 598,\n",
       " 'boo': 599,\n",
       " 'noamnesty': 600,\n",
       " 'himscrvikr': 601,\n",
       " 'healthcare': 602,\n",
       " 'used': 603,\n",
       " 'low': 604,\n",
       " 'income': 605,\n",
       " 'affordable': 606,\n",
       " 'isn': 607,\n",
       " '“': 608,\n",
       " 'thedemocrats': 609,\n",
       " 'looking': 610,\n",
       " 'lot': 611,\n",
       " '”': 612,\n",
       " 'facebook': 613,\n",
       " 'instagram': 614,\n",
       " 'ironic': 615,\n",
       " 'dreamdefenders': 616,\n",
       " 'read': 617,\n",
       " 'debates': 618,\n",
       " 'popehat': 619,\n",
       " 'system': 620,\n",
       " 'say': 621,\n",
       " 'literally': 622,\n",
       " 'anything': 623,\n",
       " 'cheer': 624,\n",
       " 'hoping': 625,\n",
       " 'actual': 626,\n",
       " 'take': 627,\n",
       " 'control': 628,\n",
       " 'these': 629,\n",
       " 'guys': 630,\n",
       " 'write': 631,\n",
       " 'snl': 632,\n",
       " 'even': 633,\n",
       " 'kidding': 634,\n",
       " 'pissed': 635,\n",
       " 'off': 636,\n",
       " 'american': 637,\n",
       " 'put': 638,\n",
       " 'feelings': 639,\n",
       " 'least': 640,\n",
       " 'carlyfiorina': 641,\n",
       " 'stage': 642,\n",
       " 'republicans': 643,\n",
       " 'hate': 644,\n",
       " 'held': 645,\n",
       " 'zone': 646,\n",
       " 'cherry': 647,\n",
       " 'picking': 648,\n",
       " 'softball': 649,\n",
       " 'or': 650,\n",
       " 'else': 651,\n",
       " 'punch': 652,\n",
       " 'face': 653,\n",
       " 'fools': 654,\n",
       " 'without': 655,\n",
       " 'gopdeba…': 656,\n",
       " 'yeah': 657,\n",
       " 'liberal': 658,\n",
       " 'dog': 659,\n",
       " 'loved': 660,\n",
       " 'actually': 661,\n",
       " 'watch': 662,\n",
       " 'pay': 663,\n",
       " 'r…': 664,\n",
       " 'scriptures': 665,\n",
       " 'lying': 666,\n",
       " 'americans': 667,\n",
       " 'dumb': 668,\n",
       " 'their': 669,\n",
       " 'sums': 670,\n",
       " 'old': 671,\n",
       " 'as': 672,\n",
       " 'tax': 673,\n",
       " 'political': 674,\n",
       " 'corporate': 675,\n",
       " 'leaders': 676,\n",
       " 'nation': 677,\n",
       " 'struggling': 678,\n",
       " 'economy': 679,\n",
       " 'entertaining': 680,\n",
       " 'berniesanders': 681,\n",
       " 'scottwalker': 682,\n",
       " 'intelligent': 683,\n",
       " 'none': 684,\n",
       " 'feelthebern': 685,\n",
       " 'because': 686,\n",
       " 'moment': 687,\n",
       " 'o…': 688,\n",
       " 'against': 689,\n",
       " 'benefits': 690,\n",
       " 'florida': 691,\n",
       " 'ht…': 692,\n",
       " 'may': 693,\n",
       " 'video': 694,\n",
       " 'clip': 695,\n",
       " 'supreme': 696,\n",
       " 'ronald': 697,\n",
       " 'jesus': 698,\n",
       " 'reagan': 699,\n",
       " 'lots': 700,\n",
       " 'twitter': 701,\n",
       " 'lrihendry': 702,\n",
       " 'always': 703,\n",
       " 'tell': 704,\n",
       " 'truth': 705,\n",
       " 'said': 706,\n",
       " 'remedy': 707,\n",
       " 'hangover': 708,\n",
       " 'ff': 709,\n",
       " 'fakedansavage': 710,\n",
       " 'stood': 711,\n",
       " 'quite': 712,\n",
       " 'batsask': 713,\n",
       " 'pretend': 714,\n",
       " 'surprised': 715,\n",
       " 'legitimate': 716,\n",
       " 'look': 717,\n",
       " 'nails': 718,\n",
       " 'newsflash': 719,\n",
       " 'plan': 720,\n",
       " 'imagining': 721,\n",
       " 'giving': 722,\n",
       " 'union': 723,\n",
       " 'ok': 724,\n",
       " 'maybe': 725,\n",
       " 'treat': 726,\n",
       " 'same': 727,\n",
       " 'except': 728,\n",
       " 'course': 729,\n",
       " 'reality': 730,\n",
       " 'earth': 731,\n",
       " 'yaakovschapiro': 732,\n",
       " 'analysis': 733,\n",
       " '=': 734,\n",
       " 'strong': 735,\n",
       " 'leadership': 736,\n",
       " 'ideas': 737,\n",
       " 'solutions': 738,\n",
       " 'super': 739,\n",
       " 'mistake': 740,\n",
       " 'fun': 741,\n",
       " 'seeing': 742,\n",
       " 'deep': 743,\n",
       " 'us': 744,\n",
       " 'become': 745,\n",
       " 'disturbing': 746,\n",
       " 'lukewearechange': 747,\n",
       " 'where': 748,\n",
       " 'evidence': 749,\n",
       " 'hacks': 750,\n",
       " 'talk': 751,\n",
       " 'comes': 752,\n",
       " 'syria': 753,\n",
       " 'worst': 754,\n",
       " 'episode': 755,\n",
       " 'apprentice': 756,\n",
       " 'ever': 757,\n",
       " 'seen': 758,\n",
       " 'tune': 759,\n",
       " 'gopdebacle': 760,\n",
       " 'frightening': 761,\n",
       " 'yet': 762,\n",
       " 'billion': 763,\n",
       " 'dollars': 764,\n",
       " 'trillion': 765,\n",
       " 'teaparty': 766,\n",
       " 'fit': 767,\n",
       " 'looks': 768,\n",
       " 'tv': 769,\n",
       " 'every': 770,\n",
       " 'tough': 771,\n",
       " 'hearing': 772,\n",
       " 'road': 773,\n",
       " 'nomination': 774,\n",
       " 'chrisstirewalt': 775,\n",
       " 'turned': 776,\n",
       " 'solid': 777,\n",
       " 'performance': 778,\n",
       " 'cruzcrew': 779,\n",
       " 'consensus': 780,\n",
       " 'brilliant': 781,\n",
       " 'party': 782,\n",
       " 'est': 783,\n",
       " 'rove': 784,\n",
       " 'statist': 785,\n",
       " 'democracymatrz': 786,\n",
       " 'buying': 787,\n",
       " 'while': 788,\n",
       " 'declaring': 789,\n",
       " 'campaign': 790,\n",
       " 'finance': 791,\n",
       " 'freedomjames': 792,\n",
       " 'forget': 793,\n",
       " 'supported': 794,\n",
       " 'amnesty': 795,\n",
       " 'insulted': 796,\n",
       " 'budget': 797,\n",
       " 'taxes': 798,\n",
       " 'cuts': 799,\n",
       " 'z': 800,\n",
       " 'v': 801,\n",
       " 'toddstarnes': 802,\n",
       " '—': 803,\n",
       " 'disgusting': 804,\n",
       " 'moderator': 805,\n",
       " 'forgot': 806,\n",
       " 'chinese': 807,\n",
       " 'hacking': 808,\n",
       " 'started': 809,\n",
       " 'under': 810,\n",
       " 'admin': 811,\n",
       " 'anymore': 812,\n",
       " 'queen': 813,\n",
       " 'fashion': 814,\n",
       " 'tried': 815,\n",
       " 'himself': 816,\n",
       " 'lied': 817,\n",
       " 'twice': 818,\n",
       " 'y': 819,\n",
       " 'ears': 820,\n",
       " 'bigger': 821,\n",
       " 'churches': 822,\n",
       " 'israel': 823,\n",
       " 'lizzwinstead': 824,\n",
       " 'wants': 825,\n",
       " 'bencarson': 826,\n",
       " 'dick': 827,\n",
       " 'seemed': 828,\n",
       " 'more': 829,\n",
       " 'understand': 830,\n",
       " 'through': 831,\n",
       " 'cold': 832,\n",
       " 'open': 833,\n",
       " 'mzdivah': 834,\n",
       " 'seems': 835,\n",
       " 'blacklivesdontmatter': 836,\n",
       " 'pbo': 837,\n",
       " 'failed': 838,\n",
       " 'policies': 839,\n",
       " 'impo…': 840,\n",
       " 'stuck': 841,\n",
       " 'head': 842,\n",
       " 'marcorubio': 843,\n",
       " 'grneyedmandy': 844,\n",
       " 'conservatives': 845,\n",
       " 'daughters': 846,\n",
       " 'go': 847,\n",
       " 'next': 848,\n",
       " 'amen': 849,\n",
       " 'culture': 850,\n",
       " 'life': 851,\n",
       " 'getting': 852,\n",
       " 'shot': 853,\n",
       " 'blacklivesmatter': 854,\n",
       " 'disappointing': 855,\n",
       " 'conducting': 856,\n",
       " 'role': 857,\n",
       " 'fail': 858,\n",
       " 'sallykohn': 859,\n",
       " 'depressing': 860,\n",
       " 'hot': 861,\n",
       " 'danscavino': 862,\n",
       " 'leading': 863,\n",
       " '%': 864,\n",
       " 'stupid': 865,\n",
       " 'eating': 866,\n",
       " 'mind': 867,\n",
       " 'mike': 868,\n",
       " 'hit': 869,\n",
       " 'fat': 870,\n",
       " 'united': 871,\n",
       " 'nod': 872,\n",
       " 'black': 873,\n",
       " 'speaks': 874,\n",
       " 'asshole': 875,\n",
       " 'probably': 876,\n",
       " 'bettyfckinwhite': 877,\n",
       " 'jokes': 878,\n",
       " 'dream': 879,\n",
       " 'became': 880,\n",
       " 'nightmare': 881,\n",
       " 'justiceorelse': 882,\n",
       " 'govchristie': 883,\n",
       " 'hugged': 884,\n",
       " 'deserve': 885,\n",
       " 'morning': 886,\n",
       " 'towards': 887,\n",
       " 'thinking': 888,\n",
       " 'hilarious': 889,\n",
       " 'then': 890,\n",
       " 'realized': 891,\n",
       " 'watched': 892,\n",
       " '😂': 893,\n",
       " 'carolhello': 894,\n",
       " 'kwrcrow': 895,\n",
       " 'learn': 896,\n",
       " 'wins': 897,\n",
       " 'drudge': 898,\n",
       " 'poll': 899,\n",
       " 'drink': 900,\n",
       " 'rnc': 901,\n",
       " 'mydaughtersarmy': 902,\n",
       " 'group': 903,\n",
       " 'choice': 904,\n",
       " 'happens': 905,\n",
       " 'bodies': 906,\n",
       " 'finish': 907,\n",
       " 'dvr': 908,\n",
       " 'remembered': 909,\n",
       " 'early': 910,\n",
       " 'haven': 911,\n",
       " 'keep': 912,\n",
       " 'soul': 913,\n",
       " 'support': 914,\n",
       " 'better': 915,\n",
       " 'cdm': 916,\n",
       " 'ago': 917,\n",
       " 'nytimes': 918,\n",
       " 'words': 919,\n",
       " 'writing': 920,\n",
       " 'gone': 921,\n",
       " 'looked': 922,\n",
       " 'learned': 923,\n",
       " 'false': 924,\n",
       " 'sen': 925,\n",
       " 'former': 926,\n",
       " 'hp': 927,\n",
       " 'ceo': 928,\n",
       " 'theblaze': 929,\n",
       " 'pundits': 930,\n",
       " 'interested': 931,\n",
       " 'again': 932,\n",
       " 'trans': 933,\n",
       " 'lgbt': 934,\n",
       " '*': 935,\n",
       " 'k': 936,\n",
       " 'those': 937,\n",
       " 'abandon': 938,\n",
       " 'values': 939,\n",
       " 'candidacy': 940,\n",
       " 'patriotmom': 941,\n",
       " 'moderators': 942,\n",
       " 'santorum': 943,\n",
       " 'sec': 944,\n",
       " 'i…': 945,\n",
       " 'sad': 946,\n",
       " 'thinks': 947,\n",
       " 'desdemona': 948,\n",
       " 'unfathomable': 949,\n",
       " 'wasserman-schultz': 950,\n",
       " 'postmortem': 951,\n",
       " 'surreal': 952,\n",
       " 'surgeon': 953,\n",
       " 'general': 954,\n",
       " 'strange': 955,\n",
       " 'creepy': 956,\n",
       " 'letter': 957,\n",
       " 'screen': 958,\n",
       " 'spoke': 959,\n",
       " 'courtesy': 960,\n",
       " 'smart': 961,\n",
       " 'alive': 962,\n",
       " 'climate': 963,\n",
       " 'ur': 964,\n",
       " 'cat_': 965,\n",
       " 'foxtv': 966,\n",
       " 'holding': 967,\n",
       " 'millions': 968,\n",
       " 'msn': 969,\n",
       " 'abc': 970,\n",
       " 'cbs': 971,\n",
       " 'oops': 972,\n",
       " 'jq': 973,\n",
       " 'fb': 974,\n",
       " 'xm': 975,\n",
       " 'everything': 976,\n",
       " 'hoped': 977,\n",
       " 'internet': 978,\n",
       " 'amazing': 979,\n",
       " 'gopclowncar': 980,\n",
       " 'didnt': 981,\n",
       " 'goodbye': 982,\n",
       " 'dude': 983,\n",
       " 'nuts': 984,\n",
       " 'blaming': 985,\n",
       " 'misogynistic': 986,\n",
       " 'proved': 987,\n",
       " 'bit': 988,\n",
       " 'biased': 989,\n",
       " 'obnoxious': 990,\n",
       " 'candy': 991,\n",
       " 'crowley': 992,\n",
       " 'cnn': 993,\n",
       " 'chair': 994,\n",
       " 'dnc': 995,\n",
       " 'democrat': 996,\n",
       " 'socialist': 997,\n",
       " 'called': 998,\n",
       " 'pig': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qo-WXkDBfkHf"
   },
   "source": [
    "## Model Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4vIjTqefkHg"
   },
   "source": [
    "#### Word representations\n",
    "Next, we neeed to convert each word/token in the sentences into its index in the vocabulary so that pytorch can use it. We do this for both train and test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvBthg06fkHh"
   },
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "rL3LK19wfkHi"
   },
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, vocab_index, df, label = 'sentiment'):\n",
    "        self.vocab_index = vocab_index\n",
    "        self.df = df\n",
    "        self.label = label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        sentence = self.df.loc[key, 'text']\n",
    "        sentence = re.sub('[\\\\(\\[#.!?,\\'\\/\\])0-9]', ' ', sentence)\n",
    "        token_indices = np.array([self.vocab_index[word] if word in self.vocab_index else self.vocab_index['<UNK>'] for word in word_tokenize(sentence.lower())])\n",
    "        return (torch.tensor(token_indices) , self.df.loc[key, self.label])\n",
    "\n",
    "\n",
    "def pad_collate(batch):\n",
    "    (xx, yy) = zip(*batch)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    \n",
    "    # I want to    eat an     apple\n",
    "    # I am   going to  sleep  PAD  \n",
    "    # batch_first: output will be in B x T x * if True, or in T x B x * otherwise\n",
    "    xx_pad = pad_sequence(xx, batch_first=True, padding_value=len(vocabulary)-1)\n",
    "\n",
    "    return torch.as_tensor(xx_pad), torch.as_tensor(x_lens), torch.LongTensor(yy)\n",
    "    \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "# shuffle: set to True to have the data reshuffled at every epoch\n",
    "train_loader = DataLoader(TweetDataset(vocab_indices, train_data),\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          collate_fn = pad_collate)\n",
    "test_loader = DataLoader(TweetDataset(vocab_indices, test_data),\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True,\n",
    "                         collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M013551OhIWJ"
   },
   "source": [
    "Let's get a general idea of what an instance of training batch will be like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pojbeV6LhIWJ",
    "outputId": "52cdfb07-69ea-4cd3-f526-46a92faedaa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************Padded sequence*********************************\n",
      "tensor([  30,   17, 3067,   31,   48,  319, 2201, 3067, 3067,  264,   27, 3067,\n",
      "        1965,    9, 3067,  152,  266,   32, 3068, 3068, 3068, 3068, 3068, 3068,\n",
      "        3068, 3068, 3068, 3068, 3068, 3068])\n",
      "*******************************Length of sequence*******************************\n",
      "tensor([18, 16, 15, 23, 27, 25, 27, 26, 22, 17, 16, 12, 26, 11, 28, 25, 30, 21,\n",
      "        24, 30, 28, 23, 24, 21, 19, 20, 21, 11, 12,  5, 21, 21])\n",
      "*******************************Label of sequence********************************\n",
      "tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "sample_input = next(iter(train_loader))\n",
    "print(\"Padded sequence\".center(80, '*'))\n",
    "print(sample_input[0][0])\n",
    "print(\"Length of sequence\".center(80, '*'))\n",
    "print(sample_input[1])\n",
    "print(\"Label of sequence\".center(80, '*'))\n",
    "print(sample_input[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0jn38-qfkHl"
   },
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCFvm152fkHu"
   },
   "source": [
    "- Each input word is represented by a vector of dimension ```embedding_dim```. Check out ```nn.Embedding``` to see how to initialize embeddings randomly.\n",
    "- Your model should take the following input parameters\n",
    "    - ```hidden_dim```: The number of features in the hidden state h of your RNN layer\n",
    "    - ```output_dim```: Number of output classes\n",
    "    - ```vocab_size``` Size of your vocabulary. \n",
    "    - ```embedding_dim```: Dimension of word embeddings\n",
    "- Your model should consist of an RNN layer (you can use either ```nn.RNN``` or ```nn.LSTM```) followed by a linear layer.\n",
    "- $h_{0}$ (and $c$ if you use LSTM) should be initialized as a zero vector of dimension ```hidden_dim```. You might want to check out ```nn.Parameter```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "IX6Hf7xqfkHm"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, hidden_dim, output_dim, \n",
    "                 vocab_size, embedding_dim, rnn='LSTM'):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim, padding_idx=vocab_size-1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_fn = rnn\n",
    "        assert self.rnn_fn in ['LSTM', 'RNN']\n",
    "        self.rnn = getattr(nn, rnn)(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x, x_len):\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        # output:  tensor containing the output features (h_t) from the last layer of the RNN, tensor containing the hidden state for t = seq_len.\n",
    "        # pack_padded_sequence: Packs a Tensor containing padded sequences of variable length.\n",
    "        # enforce_sorted: if True, the input is expected to contain sequences sorted by length in a decreasing order. If False, the input will get sorted unconditionally.\n",
    "        _, last_hidden = self.rnn(pack_padded_sequence(x, x_len.to('cpu'), batch_first=True, enforce_sorted=False))\n",
    "        if self.rnn_fn == 'LSTM':\n",
    "            # (h,c)\n",
    "            # c_0: tensor containing the initial cell state for each element in the batch.\n",
    "            last_hidden = last_hidden[0]\n",
    "        out = self.fc(last_hidden.view(-1, self.hidden_dim))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8Qrx4KYfkHp"
   },
   "source": [
    "### Train and validation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5nc3scsLfkHq"
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader=train_loader, test_loader=test_loader, \n",
    "          learning_rate=0.001, num_epoch=10, print_every=100):\n",
    "    # Training steps\n",
    "    start_time = time.time()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=10**(-5))\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "        for i, (data, data_len, labels) in enumerate(train_loader):\n",
    "            data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
    "            outputs = model(data, data_len)\n",
    "            model.zero_grad()\n",
    "            loss = loss_fn(outputs.squeeze(), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "             # report performance\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                    epoch, i + 1, len(train_loader), loss.item()))     \n",
    "    \n",
    "    # Evaluate after every epochh\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        truths = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, data_len, labels) in enumerate(test_loader):\n",
    "                data, data_len, labels = data.to(device), data_len.to(device), labels.to(device)\n",
    "                outputs = model(data, data_len)\n",
    "                pred = outputs.data.max(-1)[1]\n",
    "                predictions += list(pred.cpu().numpy())\n",
    "                truths += list(labels.cpu().numpy())\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum()\n",
    "                \n",
    "            acc = (100 * correct / total)\n",
    "            auc = roc_auc_score(truths, predictions)\n",
    "            elapse = time.strftime('%H:%M:%S', time.gmtime(int((time.time() - start_time))))\n",
    "            print('Test set | Accuracy: {:6.4f} | AUC: {:4.2f} | time elapse: {:>9}'.format(\n",
    "                acc, auc, elapse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQMLwClifkHv"
   },
   "source": [
    "Run the code block below to check your model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "seXmatFEfkHw",
    "outputId": "9c97cc83-5758-4843-d2fb-a8f5e5c47b17",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.5967\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.4869\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.6579\n",
      "Test set | Accuracy: 80.9879 | AUC: 0.58 | time elapse:  00:00:01\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.5395\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.6252\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.3192\n",
      "Test set | Accuracy: 81.9199 | AUC: 0.62 | time elapse:  00:00:02\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.3423\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.4585\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.3538\n",
      "Test set | Accuracy: 83.0382 | AUC: 0.66 | time elapse:  00:00:04\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.3827\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.2538\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.4507\n",
      "Test set | Accuracy: 84.0634 | AUC: 0.68 | time elapse:  00:00:05\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.2827\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.4300\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.3569\n",
      "Test set | Accuracy: 83.8770 | AUC: 0.69 | time elapse:  00:00:07\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.3984\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.4078\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.4491\n",
      "Test set | Accuracy: 84.2498 | AUC: 0.67 | time elapse:  00:00:08\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.0768\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.3027\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.2245\n",
      "Test set | Accuracy: 83.2246 | AUC: 0.73 | time elapse:  00:00:09\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.4003\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.2493\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.1789\n",
      "Test set | Accuracy: 83.9702 | AUC: 0.69 | time elapse:  00:00:11\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.1315\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.1742\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.1377\n",
      "Test set | Accuracy: 83.6906 | AUC: 0.72 | time elapse:  00:00:12\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.1701\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.1275\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.1237\n",
      "Test set | Accuracy: 83.8770 | AUC: 0.71 | time elapse:  00:00:14\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "rnn_model = RNN(40, 2, len(vocabulary), 50, rnn='RNN').to(device)\n",
    "train(rnn_model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gzqv_ivfkHz",
    "outputId": "c7a9bc4e-f313-4425-d3f1-6b10d6bda631",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.3871\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.7016\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.5600\n",
      "Test set | Accuracy: 81.2675 | AUC: 0.58 | time elapse:  00:00:02\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.4215\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.4763\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.2787\n",
      "Test set | Accuracy: 84.2498 | AUC: 0.66 | time elapse:  00:00:04\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.4776\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.2461\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.1980\n",
      "Test set | Accuracy: 83.9702 | AUC: 0.67 | time elapse:  00:00:07\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.3883\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.2145\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.2379\n",
      "Test set | Accuracy: 85.0885 | AUC: 0.70 | time elapse:  00:00:09\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.3391\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.2578\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.2347\n",
      "Test set | Accuracy: 85.3681 | AUC: 0.71 | time elapse:  00:00:11\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.3361\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.3735\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.2040\n",
      "Test set | Accuracy: 84.3430 | AUC: 0.73 | time elapse:  00:00:14\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.2651\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.0630\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.2094\n",
      "Test set | Accuracy: 85.2749 | AUC: 0.72 | time elapse:  00:00:16\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.2896\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.1932\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.2385\n",
      "Test set | Accuracy: 84.5294 | AUC: 0.71 | time elapse:  00:00:19\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.1285\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.0711\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.2084\n",
      "Test set | Accuracy: 82.2926 | AUC: 0.71 | time elapse:  00:00:21\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.1074\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.1466\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.0855\n",
      "Test set | Accuracy: 84.0634 | AUC: 0.72 | time elapse:  00:00:24\n"
     ]
    }
   ],
   "source": [
    "lstm_model = RNN(40, 2, len(vocabulary), 50, rnn='LSTM').to(device)\n",
    "train(lstm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pz5dadxpfkH2"
   },
   "source": [
    "### Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "gfn9GiRfqnx2"
   },
   "outputs": [],
   "source": [
    "def sentences_to_padded_index_sequences(words, sentences):\n",
    "    for i, s in enumerate(sentences):\n",
    "        # h tokens\n",
    "        token_indices = np.array([words[w] if w in words else words['<UNK>'] for w in word_tokenize(s.lower())])\n",
    "    return token_indices, len(token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "CFHMg5TefkH3"
   },
   "outputs": [],
   "source": [
    "def test_sentence(sentence, model):\n",
    "    model.eval()\n",
    "    test_tensor, len_sent = sentences_to_padded_index_sequences(vocab_indices, [sentence])\n",
    "    score = model(torch.LongTensor(test_tensor.astype(int)).unsqueeze(0).to(device), torch.as_tensor([len_sent]).to(device)).cpu().data.numpy().squeeze()\n",
    "    label = np.argmax(score)\n",
    "    return (\"positive\" if label == 1 else \"negative\", score[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAFUwL-MfkH7",
    "outputId": "d035146b-d8d9-44b0-c1c8-7557100b5a8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('positive', 0.7944534)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence(\"Today's weather is so good!\", rnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j689ttEqfkH_",
    "outputId": "e4dbedd4-0ee9-46e6-815c-a1b7e6c551e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('negative', 1.5269691)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence(\"The way he talked to me is awful\", rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNnHlWsFfkIB"
   },
   "source": [
    "## Word Embeddings and How to Use Them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1MwIJMafkIC"
   },
   "source": [
    "When using deep learning methods on NLP tasks, we usually utilize [word embedding](https://en.wikipedia.org/wiki/Word_embedding). To put it briefly, word embedding represent words, or tokens, in a vocabulary as a distributed numerical vector. There are a lot of methods to obtain a word embedding, with some of the most famous shallow models being Word2Vec, GloVe, and FastText while the deeper models are BERT, RoBERTa, T5. It is not difficult to find a general purpose word embedding trained by one of the aforementioned methods on the Internet that's been trained with a massive amount of data. It is usually a good idea to use these pre-trained embedding to save yourself some time and computing resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta-GpbQpfkID"
   },
   "source": [
    "In this lab, we will be using the [GloVe embedding](https://nlp.stanford.edu/projects/glove/) developed by Stanford,  one of the state-of-the-art word embedding. Please download the file ```glove.6B.50d.txt``` [here](https://drive.google.com/file/d/1JweINiA5JvTNLTm663LH8OdWssK2Kcid/view?usp=sharing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find similar words\n",
    "\n",
    "The word embedding vectors can help us find words with similar meanings. Word similarities can be measured by [Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "6ErZicnmfkIE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jq/ln84hbtj50nf19ch201dd3br0000gn/T/ipykernel_13216/2910554770.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  _ = glove2word2vec('./glove.6B.50d.txt', 'tmp_file')\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "# load embedding\n",
    "\n",
    "_ = glove2word2vec('./glove.6B.50d.txt', 'tmp_file')\n",
    "glove_embedding = KeyedVectors.load_word2vec_format('tmp_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PD5uELzwfkIO",
    "outputId": "e14989bf-f896-45fe-dee4-8f818b533c05"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('college', 0.9344995617866516),\n",
       " ('schools', 0.868353009223938),\n",
       " ('campus', 0.8472231030464172),\n",
       " ('graduate', 0.8460071682929993),\n",
       " ('elementary', 0.8369437456130981)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.similar_by_word('school', topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SiLnfaRHfkIR"
   },
   "source": [
    "### Word arithmetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWh4FHadfkIS",
    "outputId": "d3f6eb5c-eec5-489b-8fcf-731c158acea7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('worst', 0.8109661340713501)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.similar_by_word(glove_embedding['worse'] - glove_embedding['better'] + glove_embedding['best'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVI_p6jDfkIY"
   },
   "source": [
    "### Train an LSTM model with GloVe embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKwQwYI6BIwg"
   },
   "source": [
    "- Create the new 'dictionary' to send it into our dataset class\n",
    "- Load the GloVe Embedding into our nn.Embedding layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t-3_a6UkzZwa",
    "outputId": "9146ac08-6a03-4c18-ab89-54ad8107bfdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:03, 133295.33it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "# load embedding\n",
    "emb_dim = 50\n",
    "with open('./glove.6B.50d.txt') as f:\n",
    "    glove_embedding = []\n",
    "    words = {}\n",
    "\n",
    "    for i, line in tqdm(enumerate(f)):\n",
    "        s = line.split()\n",
    "        glove_embedding.append(np.asarray(s[1:]))\n",
    "        \n",
    "        words[s[0]] = len(words)\n",
    " \n",
    "        \n",
    "# add unknown to word and char\n",
    "glove_embedding.append(np.random.rand(emb_dim))\n",
    "words[\"<UNK>\"] = len(words)\n",
    "\n",
    "# add padding\n",
    "glove_embedding.append(np.zeros(emb_dim))\n",
    "words[\"<PAD>\"] = len(words)\n",
    "\n",
    "\n",
    "glove_embedding = np.array(glove_embedding).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400002"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "oxEG3l-izy0N"
   },
   "outputs": [],
   "source": [
    "train_loader_glove = DataLoader(TweetDataset(words, train_data),\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                shuffle=True,\n",
    "                                collate_fn = pad_collate)\n",
    "test_loader_glove = DataLoader(TweetDataset(words, test_data),\n",
    "                               batch_size=BATCH_SIZE,\n",
    "                               shuffle=True,\n",
    "                               collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64PinsMTfkIk",
    "outputId": "f4e06742-6a8a-4df3-f540-de625c51993d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4180,  0.2497, -0.4124,  ..., -0.1841, -0.1151, -0.7858],\n",
       "        [ 0.0134,  0.2368, -0.1690,  ..., -0.5666,  0.0447,  0.3039],\n",
       "        [ 0.1516,  0.3018, -0.1676,  ..., -0.3565,  0.0164,  0.1022],\n",
       "        ...,\n",
       "        [ 0.0726, -0.5139,  0.4728,  ..., -0.1891, -0.5902,  0.5556],\n",
       "        [ 0.0560,  0.8742,  0.3537,  ...,  0.2650,  0.6108,  0.7820],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model = RNN(40, 2, len(glove_embedding), 50, rnn='LSTM')\n",
    "glove_model.emb.weight.data.copy_(torch.from_numpy(glove_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set | epoch:   0 |    100/   302 batches | Loss: 0.4760\n",
      "Train set | epoch:   0 |    200/   302 batches | Loss: 0.4253\n",
      "Train set | epoch:   0 |    300/   302 batches | Loss: 0.3312\n",
      "Test set | Accuracy: 80.9879 | AUC: 0.58 | time elapse:  00:00:11\n",
      "Train set | epoch:   1 |    100/   302 batches | Loss: 0.2679\n",
      "Train set | epoch:   1 |    200/   302 batches | Loss: 0.4546\n",
      "Train set | epoch:   1 |    300/   302 batches | Loss: 0.3785\n",
      "Test set | Accuracy: 84.4362 | AUC: 0.67 | time elapse:  00:00:23\n",
      "Train set | epoch:   2 |    100/   302 batches | Loss: 0.3231\n",
      "Train set | epoch:   2 |    200/   302 batches | Loss: 0.5888\n",
      "Train set | epoch:   2 |    300/   302 batches | Loss: 0.4438\n",
      "Test set | Accuracy: 81.5471 | AUC: 0.76 | time elapse:  00:00:35\n",
      "Train set | epoch:   3 |    100/   302 batches | Loss: 0.4040\n",
      "Train set | epoch:   3 |    200/   302 batches | Loss: 0.3123\n",
      "Train set | epoch:   3 |    300/   302 batches | Loss: 0.3777\n",
      "Test set | Accuracy: 83.7838 | AUC: 0.72 | time elapse:  00:00:47\n",
      "Train set | epoch:   4 |    100/   302 batches | Loss: 0.3932\n",
      "Train set | epoch:   4 |    200/   302 batches | Loss: 0.4302\n",
      "Train set | epoch:   4 |    300/   302 batches | Loss: 0.2789\n",
      "Test set | Accuracy: 83.8770 | AUC: 0.72 | time elapse:  00:00:58\n",
      "Train set | epoch:   5 |    100/   302 batches | Loss: 0.0821\n",
      "Train set | epoch:   5 |    200/   302 batches | Loss: 0.5515\n",
      "Train set | epoch:   5 |    300/   302 batches | Loss: 0.2325\n",
      "Test set | Accuracy: 82.3858 | AUC: 0.75 | time elapse:  00:01:10\n",
      "Train set | epoch:   6 |    100/   302 batches | Loss: 0.1473\n",
      "Train set | epoch:   6 |    200/   302 batches | Loss: 0.2223\n",
      "Train set | epoch:   6 |    300/   302 batches | Loss: 0.0738\n",
      "Test set | Accuracy: 84.3430 | AUC: 0.71 | time elapse:  00:01:22\n",
      "Train set | epoch:   7 |    100/   302 batches | Loss: 0.0811\n",
      "Train set | epoch:   7 |    200/   302 batches | Loss: 0.0578\n",
      "Train set | epoch:   7 |    300/   302 batches | Loss: 0.0423\n",
      "Test set | Accuracy: 84.3430 | AUC: 0.74 | time elapse:  00:01:33\n",
      "Train set | epoch:   8 |    100/   302 batches | Loss: 0.0489\n",
      "Train set | epoch:   8 |    200/   302 batches | Loss: 0.0746\n",
      "Train set | epoch:   8 |    300/   302 batches | Loss: 0.3867\n",
      "Test set | Accuracy: 82.4790 | AUC: 0.74 | time elapse:  00:01:45\n",
      "Train set | epoch:   9 |    100/   302 batches | Loss: 0.1129\n",
      "Train set | epoch:   9 |    200/   302 batches | Loss: 0.0648\n",
      "Train set | epoch:   9 |    300/   302 batches | Loss: 0.0204\n",
      "Test set | Accuracy: 82.9450 | AUC: 0.71 | time elapse:  00:01:56\n"
     ]
    }
   ],
   "source": [
    "train(glove_model.to(device), train_loader=train_loader_glove, test_loader=test_loader_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
